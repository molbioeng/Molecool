{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original Database formatting\n",
    "Use this Notebook for initial formatting of the database.\n",
    "\n",
    "Checklist:\n",
    "- Remove patients without ECG signal from data description file (done)\n",
    "- Upload (and format) all ECG signals to new database (done)\n",
    "- Modularise code (done)\n",
    "- Write checks in the functions - should be able to check for errors in metadata and flag this\n",
    "- Ensure all of the metadata information is in a standard form\n",
    "\n",
    "For more information on the pyECG module: \n",
    "https://www.researchgate.net/publication/331012096_PyECG_A_software_tool_for_the_analysis_of_the_QT_interval_in_the_electrocardiogram\n",
    "\n",
    "https://pypi.org/project/pyECG/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pyecg import ECGRecord\n",
    "import json\n",
    "import pyarrow.feather as feather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WFDB file formats\n",
    "### Header files\n",
    "The record line\n",
    "E.g. S0250ECG 8 1000 1256376 15:26:59 2006\n",
    "- Record name: S0250ECG\n",
    "- Number of signals: 8\n",
    "- Sampling frequency (in samples per second per signal): 1000\n",
    "- Number of samples per signal: 1256376\n",
    "- Base time (time of day corresponding with the beginning of the record): 15:26:59\n",
    "- Base date: 2006 (this is in the wrong format in the database, a quick fix is just removing it from the header but this may be annoying)\n",
    "\n",
    "Signal specification lines\n",
    "Each of the non-empty, non-comment lines following the record line represent 1 signal.\n",
    "E.g. S0250ECG.dat 16 1(0)/uV 16 0 -88 25184 0 ecg_0\n",
    "- File name (of where the signal is stored): S0250ECG.dat\n",
    "- Format: 16-bit-amplitudes (see documentation on signal files)\n",
    "- ADC gain: 1 (uV)\n",
    "- Baseline: 0 (uV)\n",
    "- ADC resolution (bits): 16\n",
    "- ADC zero: 0\n",
    "- initial value (of signal): -88\n",
    "- checksum (used to verify that the file hasn't been corrupted): 25184\n",
    "- block size: 0\n",
    "- Description: 'ecg_0'\n",
    "\n",
    "For more information on header files: https://archive.physionet.org/physiotools/wag/header-5.htm\n",
    "\n",
    "### Signal files...\n",
    "Info can be found here: https://archive.physionet.org/physiotools/wag/signal-5.htm\n",
    "\n",
    "## Creating Metadata Files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting root directory of database to source data from\n",
    "root = 'G:\\My Drive\\Molecool\\Databases\\Database1'\n",
    "droot = 'D:\\Molecool\\og_database'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The variables of interest in this database\n",
    "general = ['Group', 'Diabetes Duration','age','BMI','Hb A1C%','CRP (mg/L)','Neuropathy AUTONOMIC SYMPTOMS','WBC K/uL','RBC m/uL','Hgb g/dL','GLUCOSE mg/dL','URINE CREAT mg/dL','URINE ALBUMIN mg/dL', 'CHOLESTmg/dL','LDL CALCmg/dL','Retinopathy Grading']\n",
    "small = ['Group', 'Diabetes Duration','age','BMI','Hb A1C%','CRP (mg/L)','Neuropathy AUTONOMIC SYMPTOMS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:\\\\Molecool\\\\og_database/data_description/GE-75_files_per_subject.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-ae58c1d04e1c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Creating the large metadata file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# List of patients with ECG readings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf_csv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdroot\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/data_description/GE-75_files_per_subject.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'latin-1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mdf_csv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Subject ID'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdf_csv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_csv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Patient IDs not uniformly entered\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\thefr\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\thefr\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\thefr\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\thefr\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\thefr\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1872\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1873\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1874\u001b[1;33m                 \u001b[0msrc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1875\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1876\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:\\\\Molecool\\\\og_database/data_description/GE-75_files_per_subject.csv'"
     ]
    }
   ],
   "source": [
    "# Creating the large metadata file\n",
    "# List of patients with ECG readings \n",
    "df_csv = pd.read_csv((droot + '/data_description/GE-75_files_per_subject.csv'), encoding = 'latin-1')\n",
    "df_csv.set_index('Subject ID', inplace=True)\n",
    "df_csv.index = df_csv.index.str.upper() # Patient IDs not uniformly entered\n",
    "df_csv = df_csv.iloc[: , 1:-3] #Drop last 3 columns as well as the group column\n",
    "df_csv = df_csv.loc[(df_csv!=0).any(1)] # Keep patients with data associated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create folders for all of the patients with data\n",
    "for sub in df_csv.index:\n",
    "    path = os.path.join(root, sub)\n",
    "    os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all other patient variables into a dataframe\n",
    "df_meta = pd.read_csv((droot + '/data_description/GE-75_data_summary_table.csv'), encoding = 'latin-1')\n",
    "df_meta.set_index('patient ID', inplace=True)\n",
    "df_meta.index = df_meta.index.str.upper()\n",
    "df_meta = df_meta[general] #Only taking variables of interest\n",
    "df_meta = pd.concat([df_meta, df_csv], axis=1).reindex(df_csv.index) #Now only use patients with data associated, and...\n",
    "#...combine the two dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 8 Controls and 46 Diabetics from the 88 initial total\n",
    "# # Now saving this as a new file for future use\n",
    "df_meta.to_json((root + '/LMeta.json'), orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Creating and saving small metadata file\n",
    "df_meta = df_meta[small]\n",
    "\n",
    "df_meta.to_json((root + '/SMeta.json'), orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking that data has been saved correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening JSON file\n",
    "f = open(root + '/formatted_data/LMeta.json')\n",
    " \n",
    "# returns JSON object as\n",
    "# a dictionary\n",
    "data = json.load(f)\n",
    " \n",
    "# Iterating through the json\n",
    "# list\n",
    "print(json.dumps(data, indent=4))\n",
    " \n",
    "# Closing file\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data into feather files\n",
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metadata(header, path):\n",
    "\"\"\"Create metadata file from the header. Saves total number of samples in the signal and time of the day the signal was started in json file for the patient\n",
    "        \n",
    "    Inputs: header file (header), patient folder (path)\n",
    "    Outputs: None\"\"\"\n",
    " \n",
    "    f_line = header.readline().split()\n",
    "    if len(f_line) < 5:\n",
    "        d = {'Length of reading':f_line[3], 'Sampling rate': f_line[2], 'Error Flag': False, 'Error Type': 'No error'}\n",
    "    else:\n",
    "        d = {'Length of reading':f_line[3], 'Start time': f_line[4], 'Sampling rate': f_line[2], 'Error Flag': False, 'Error Type': 'No error'}\n",
    "    #os.mkdir(path)\n",
    "    with open((path + '\\\\Meta.json'), \"w\") as outfile:\n",
    "        json.dump(d, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feather(header_path, lead_names, path):\n",
    "    \"\"\"Create Feather file using header file to extract signal from .DAT file\n",
    "        \n",
    "        Inputs: path of header file (header_path), list of names of the individual signal leads we want to isolate (lead_names),...\n",
    "        patient folder path (path) \n",
    "        Outputs: None\"\"\"\n",
    "    df = pd.DataFrame() \n",
    "    record = ECGRecord.from_wfdb(header_path)\n",
    "    \n",
    "    #Using pyECG Library\n",
    "    for lead in lead_names:\n",
    "        signal = record.get_lead(lead)\n",
    "        df[lead] = pd.Series(signal)\n",
    "            \n",
    "    feather.write_feather(df, (path + '/ECG.ftr'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_samples(dpath, ecg_lead_names, folder_name, just_metadata):\n",
    "    \"\"\"Cycle through the files in the folder specified (dpath), create json metadata file...\n",
    "    ...and feather signal file for each. Save these in the folder associated with the patient. \n",
    "        \n",
    "        Inputs: dpath, names of ecg leads for signal to be extracted from (ecg_lead_names), name of folder of correct type of ECG\n",
    "    signal we want to extract (e.g. 'holter', 'hut', 'sts'), bool option to just update the metadata and not the ftr file (just_metadata)\n",
    "        Outputs: None\"\"\"\n",
    "    files = sorted(os.listdir(dpath))\n",
    "\n",
    "    for file in files: #Cycle through files in the database\n",
    "        if file.endswith('.hea'):\n",
    "            #Reading and storing the data into structure \n",
    "            hea_path = dpath + file #Change the location of the file or folder.\n",
    "            f = open(hea_path, \"r\")\n",
    "            pat_name = file[:5].upper()\n",
    "            print('Reading data for subject ' + pat_name)\n",
    "            header = open(hea_path, \"r\")\n",
    "            path = root + '\\\\' + pat_name + '\\\\' + folder_name #Patient folder\n",
    "            #os.mkdir(path)\n",
    "            create_metadata(header, path)\n",
    "            if not just_metadata:\n",
    "                create_feather(hea_path, ecg_lead_names, path)\n",
    "\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Data from ECGs to new database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data for subject S0250\n",
      "Reading data for subject S0256\n",
      "Reading data for subject S0273\n",
      "Reading data for subject S0282\n",
      "Reading data for subject S0283\n",
      "Reading data for subject S0287\n",
      "Reading data for subject S0288\n",
      "Reading data for subject S0292\n",
      "Reading data for subject S0296\n",
      "Reading data for subject S0300\n",
      "Reading data for subject S0301\n",
      "Reading data for subject S0304\n",
      "Reading data for subject S0308\n",
      "Reading data for subject S0310\n",
      "Reading data for subject S0312\n",
      "Reading data for subject S0314\n",
      "Reading data for subject S0315\n",
      "Reading data for subject S0316\n",
      "Reading data for subject S0317\n",
      "Reading data for subject S0318\n",
      "Reading data for subject S0326\n",
      "Reading data for subject S0327\n",
      "Reading data for subject S0339\n",
      "Reading data for subject S0342\n",
      "Reading data for subject S0349\n",
      "Reading data for subject S0365\n",
      "Reading data for subject S0366\n",
      "Reading data for subject S0368\n",
      "Reading data for subject S0372\n",
      "Reading data for subject S0381\n",
      "Reading data for subject S0382\n",
      "Reading data for subject S0390\n",
      "Reading data for subject S0392\n",
      "Reading data for subject S0398\n",
      "Reading data for subject S0403\n",
      "Reading data for subject S0405\n",
      "Reading data for subject S0406\n",
      "Reading data for subject S0409\n",
      "Reading data for subject S0411\n",
      "Reading data for subject S0416\n",
      "Reading data for subject S0420\n",
      "Reading data for subject S0423\n",
      "Reading data for subject S0424\n",
      "Reading data for subject S0426\n",
      "Reading data for subject S0427\n",
      "Reading data for subject S0430\n",
      "Reading data for subject S0432\n",
      "Reading data for subject S0433\n",
      "Reading data for subject S0434\n",
      "Reading data for subject S0435\n",
      "Reading data for subject S0441\n",
      "Reading data for subject S0250\n",
      "Reading data for subject S0254\n",
      "Reading data for subject S0256\n",
      "Reading data for subject S0264\n",
      "Reading data for subject S0273\n",
      "Reading data for subject S0282\n",
      "Reading data for subject S0283\n",
      "Reading data for subject S0287\n",
      "Reading data for subject S0288\n",
      "Reading data for subject S0292\n",
      "Reading data for subject S0296\n",
      "Reading data for subject S0300\n",
      "Reading data for subject S0301\n",
      "Reading data for subject S0304\n",
      "Reading data for subject S0308\n",
      "Reading data for subject S0310\n",
      "Reading data for subject S0312\n",
      "Reading data for subject S0314\n",
      "Reading data for subject S0315\n",
      "Reading data for subject S0316\n",
      "Reading data for subject S0317\n",
      "Reading data for subject S0318\n",
      "Reading data for subject S0326\n",
      "Reading data for subject S0327\n",
      "Reading data for subject S0328\n",
      "Reading data for subject S0339\n",
      "Reading data for subject S0342\n",
      "Reading data for subject S0349\n",
      "Reading data for subject S0365\n",
      "Reading data for subject S0366\n",
      "Reading data for subject S0368\n",
      "Reading data for subject S0372\n",
      "Reading data for subject S0381\n",
      "Reading data for subject S0382\n",
      "Reading data for subject S0390\n",
      "Reading data for subject S0392\n",
      "Reading data for subject S0398\n",
      "Reading data for subject S0403\n",
      "Reading data for subject S0405\n",
      "Reading data for subject S0406\n",
      "Reading data for subject S0409\n",
      "Reading data for subject S0411\n",
      "Reading data for subject S0416\n",
      "Reading data for subject S0420\n",
      "Reading data for subject S0423\n",
      "Reading data for subject S0424\n",
      "Reading data for subject S0426\n",
      "Reading data for subject S0427\n",
      "Reading data for subject S0430\n",
      "Reading data for subject S0432\n",
      "Reading data for subject S0433\n",
      "Reading data for subject S0434\n",
      "Reading data for subject S0250\n",
      "Reading data for subject S0254\n",
      "Reading data for subject S0256\n",
      "Reading data for subject S0264\n",
      "Reading data for subject S0273\n",
      "Reading data for subject S0282\n",
      "Reading data for subject S0287\n",
      "Reading data for subject S0292\n",
      "Reading data for subject S0296\n",
      "Reading data for subject S0300\n",
      "Reading data for subject S0301\n",
      "Reading data for subject S0304\n",
      "Reading data for subject S0312\n",
      "Reading data for subject S0314\n",
      "Reading data for subject S0315\n",
      "Reading data for subject S0316\n",
      "Reading data for subject S0317\n",
      "Reading data for subject S0318\n",
      "Reading data for subject S0326\n",
      "Reading data for subject S0339\n",
      "Reading data for subject S0342\n",
      "Reading data for subject S0349\n",
      "Reading data for subject S0366\n",
      "Reading data for subject S0368\n",
      "Reading data for subject S0372\n",
      "Reading data for subject S0390\n",
      "Reading data for subject S0403\n",
      "Reading data for subject S0405\n",
      "Reading data for subject S0406\n",
      "Reading data for subject S0411\n",
      "Reading data for subject S0416\n",
      "Reading data for subject S0420\n",
      "Reading data for subject S0423\n",
      "Reading data for subject S0424\n",
      "Reading data for subject S0430\n",
      "Reading data for subject S0432\n",
      "Reading data for subject S0433\n",
      "Reading data for subject S0434\n",
      "Reading data for subject S0435\n"
     ]
    }
   ],
   "source": [
    "### Uploading all three ECG sample types ###\n",
    "# Looking at the overnight/12min walking data\n",
    "dpath = droot + \"\\\\ecgdata\\\\\" #Change the location of the file or folder.\n",
    "read_samples(dpath, ['ecg_0','ecg_1'], 'holter', True)\n",
    "\n",
    "# Looking at the head-up-tilt data\n",
    "dpath = droot + \"\\\\labview\\\\converted\\\\head-up-tilt\\\\\" \n",
    "read_samples(dpath, ['ecg'], 'hut', True)\n",
    "\n",
    "# Looking at the head-up-tilt data\n",
    "dpath = droot + \"\\\\labview\\\\converted\\\\sit-to-stand\\\\\" \n",
    "read_samples(dpath, ['ecg'], 'sts', True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Length of reading': '1094400', 'Sampling rate': '1000', 'Error Flag': False, 'Error Type': 'No error'}\n"
     ]
    }
   ],
   "source": [
    "#Checking that you can open the JSON file correctly\n",
    "with open('D:\\Molecool\\Databases\\Database1\\S0300\\sts\\Meta.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ecg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.068345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.066821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.066528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.064652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.062542</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ecg\n",
       "0 -0.068345\n",
       "1 -0.066821\n",
       "2 -0.066528\n",
       "3 -0.064652\n",
       "4 -0.062542"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking that you can open the feather file correctly\n",
    "df = pd.read_feather('D:\\Molecool\\Databases\\Database1\\S0300\\sts\\ECG.ftr')\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
