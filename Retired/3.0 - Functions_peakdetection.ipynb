{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51f8899e",
   "metadata": {},
   "source": [
    "# Functions for detecting peaks from ECG signals\n",
    "The file below contains all the functions needed to detect the peaks from an ECG signal acuretely. It starts by filtering and enhacning the signal, detecting the peaks based on a threshold and then correcting the detected peaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b61ff506",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "import math\n",
    "import statistics\n",
    "import neurokit2 as nk\n",
    "from scipy.signal import butter, filtfilt, iirnotch, savgol_filter\n",
    "import scipy.signal\n",
    "import peakutils.peak\n",
    "import seaborn as sns\n",
    "import pywt as pw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bc9973c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def butter_lowpass(cutoff, sample_rate, order=2):\n",
    "    \"\"\"\n",
    "    The function returns the butter indexes for a butter lowpass filter. Github https://github.com/paulvangentcom/heartrate_analysis_python/tree/0005e98618d8fc3378c03ab0a434b5d9012b1221 \n",
    "    \n",
    "    Input: cutoff-frequency from which the values will be filtered out; order-stregnth of the filter; sample_rate-rate at which the signal was sampled.\n",
    "    \n",
    "    Ouput: butter indeces.\n",
    "\n",
    "    \"\"\"\n",
    "    nyq = 0.5 * sample_rate\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "    return b, a\n",
    "\n",
    "def butter_highpass(cutoff, sample_rate, order=2):\n",
    "    \"\"\"\n",
    "    The function returns the butter indexes for a highpass filter. Github https://github.com/paulvangentcom/heartrate_analysis_python/tree/0005e98618d8fc3378c03ab0a434b5d9012b1221 \n",
    "    \n",
    "    Input: cutoff-frequency from which the values will be filtered out; order-stregnth of the filter; sample_rate-rate at which the signal was sampled.\n",
    "    \n",
    "    Ouput: butter indeces.\n",
    "\n",
    "    \"\"\"\n",
    "    nyq = 0.5 * sample_rate\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    b, a = butter(order, normal_cutoff, btype='high', analog=False)\n",
    "    return b, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "660e4298",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_signal(data, cutoff, sample_rate, order=2, filtertype='lowpass'):\n",
    "    \"\"\"\n",
    "    The function filters data in the frequency domain. \n",
    "    \n",
    "    Input: data-signal data stored in an array; cutoff-frequency from which the values will be filtered out; order-stregnth of the filter; filtertype-type of filter:\n",
    "        lowpass,highpass,bandpass,notch\n",
    "    \n",
    "    Ouput: filtered_data\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    if filtertype.lower() == 'lowpass':\n",
    "        b, a = butter_lowpass(cutoff, sample_rate, order=order)\n",
    "    elif filtertype.lower() == 'highpass':\n",
    "        b, a = butter_highpass(cutoff, sample_rate, order=order)\n",
    "    elif filtertype.lower() == 'bandpass':\n",
    "        assert type(cutoff) == tuple or list or np.array, 'if bandpass filter is specified, \\\n",
    "cutoff needs to be array or tuple specifying lower and upper bound: [lower, upper].'\n",
    "        b, a = butter_bandpass(cutoff[0], cutoff[1], sample_rate, order=order)\n",
    "    elif filtertype.lower() == 'notch':\n",
    "        b, a = iirnotch(cutoff, Q = 0.005, fs = sample_rate)\n",
    "    else:\n",
    "        raise ValueError('filtertype: %s is unknown, available are: \\\n",
    "lowpass, highpass, bandpass, and notch' %filtertype)\n",
    "\n",
    "    filtered_data = filtfilt(b, a, data)\n",
    "    \n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9c36bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_baseline_wander(data, sample_rate, cutoff=0.05):\n",
    "    \"\"\"\n",
    "    The functions removes the signal's baseline.\n",
    "    Input: data-signal stored in an array; sample_rate: sample rate in which the signal was sampled; cutoff-frequency frequency from which the values will be filtered out.\n",
    "    Output: corrected signal.\n",
    "    \"\"\"\n",
    "    return filter_signal(data = data, cutoff = cutoff, sample_rate = sample_rate,\n",
    "                         filtertype='notch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5750c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_correction(signal):\n",
    "    \"\"\"\n",
    "    The function removes a tenth of both ends from the signal by multiplying it by a half cosine function.\n",
    "    Input: signal-index of data points to be corrected.\n",
    "    Output: even signal with the corners diminished. \n",
    "    \"\"\"\n",
    "    \n",
    "    length = len(signal) # We find the length of the signal.\n",
    "    signal_10 = 0.6*length # We calculate the length of a tenth of the signal.\n",
    "    signal_90 = length - signal_10 # We calculate the difference to know the tenth corresponding to the array's final end.\n",
    "    step = (np.pi/2)/signal_10 # We find the number of values our cosine function needs to have.\n",
    "    x = np.arange(0,np.pi/2,step) # We generate half a cosine function: start,stop,step.\n",
    "    y = np.cos(x-1.57) # We shift it by pi so we can use it as a smoothening factor to apply to the values at the end of the signal.\n",
    "    cos_init = y\n",
    "    cos_end = y\n",
    "    cos_end[-1] = 0\n",
    "    cos_end[-2] = 0\n",
    "    cos_end[-3] = 0\n",
    "    \n",
    "    for index,value in enumerate(cos_init):\n",
    "        signal[index] = signal[index]*value # We perform the multiplicatoin by the signal points and the cosine points.\n",
    "\n",
    "    for index, value in enumerate(cos_end):\n",
    "        signal[-index] = value * signal[-index]\n",
    "    return signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f70d435",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(data, lower=0, upper=1024):\n",
    "    \"\"\"\n",
    "    Subfunction from the enhance_peaks section. It allows to scale the data.\n",
    "    Input: data-the signal, lower,upper-ranges that describe the scaling factor.\n",
    "    Output: scaled signal. \n",
    "    \"\"\"\n",
    "    rng = np.max(data) - np.min(data)\n",
    "    minimum = np.min(data)\n",
    "    data = (upper - lower) * ((data - minimum) / rng) + lower\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3085434f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhance_peaks(hrdata, iterations=2):\n",
    "    \"\"\"\n",
    "    The function squares the signal and, hence, enhances the peaks.\n",
    "    Input: hrdata-the signal stored in an array; iterations-the times the signal is squared.\n",
    "    Output: enhanced signal. \n",
    "    \"\"\"\n",
    "    scale_data(hrdata)\n",
    "    for i in range(iterations):\n",
    "        hrdata = np.power(hrdata, 2)\n",
    "        hrdata = scale_data(hrdata)\n",
    "    return hrdata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e79e76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothing_window(signal):\n",
    "    \"\"\"\n",
    "    The function creates a sliding window and it is used to smooth the signal out by an average mean.\n",
    "    Input: signal-the signal stored in an array.\n",
    "    Output: smoothened signal. \n",
    "    \"\"\"\n",
    "    #Define window size\n",
    "    w=30\n",
    "    #Define mask and store as an array\n",
    "    mask=np.ones((1,w))/w\n",
    "    mask=mask[0,:]\n",
    "\n",
    "    #Convolve the mask with the raw data\n",
    "    convolved_data=np.convolve(np.squeeze(signal),np.squeeze(mask),'same')\n",
    "    return convolved_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09f76e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _filtering(signal,rate):\n",
    "    \"\"\"\n",
    "    Application of a sub-sequent filtering steps, based on the Pan-Tomkins peak scalation for subsequent peak detection. \n",
    "    Input: signal-the signal stored in an array; rate-the sampling frequency at which the signal was sampled.\n",
    "    Output: filtered signal. \n",
    "    \"\"\"\n",
    "    normalised = (signal - np.min(signal)) / np.max(signal-np.min(signal)) # Signal normalisation.\n",
    "    #cos_removed = cos_correction(signal) # We remove the points located at both ends of the signal.\n",
    "    low = filter_signal(normalised, 15, rate, order=4, filtertype='lowpass') # We apply a low-pass filter.\n",
    "    high = filter_signal(low,5, rate, order=4, filtertype='highpass') # We apply a high-pass filter.\n",
    "    coeffs = pw.swt(high, wavelet = \"haar\", level=2, start_level=0, axis=-1)\n",
    "    wv = coeffs[1][1] ##2nd level detail coefficients\n",
    "    convolved = smoothing_window(wv)\n",
    "    remove = remove_baseline_wander(convolved, rate) # We remove the signal's baseline.\n",
    "    en = enhance_peaks(remove, iterations=2) # We enhance the signal twice to improve the following peak-detection.\n",
    "    smoothing = smoothing_window(en)\n",
    "\n",
    "    return smoothing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c293172a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _peakdetection(signal):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    The funtion detects the peaks based on a threshold algorithm described in the following paper:\n",
    "    https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7922324/. The algorithm has been tunned to incorporate a sliding threshold\n",
    "    that accounts for enhanced peaks. \n",
    "    \n",
    "    Input: signal-the signal we would like to obtain the peaks from.\n",
    "    Output: a dataframe containing the x and y values corresponding to the different peaks detected.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    peaks = pd.DataFrame()\n",
    "    y_points = [] # List where we are going to store the y_values of the peaks detected.\n",
    "    x_points = [] # List where we are going to store the x_values of the peaks detected.\n",
    "    temp = 0.5*((0.75*np.percentile(signal, 90))+(0.25*np.mean(signal))) #+ np.std(signal) # We define the threshold.\n",
    "    for index,point in enumerate(signal):\n",
    "        if index+1 == len(signal):\n",
    "            break\n",
    "        if point > temp: # We check if the value is bigger than the threshold. If it is the case, we set it as the new threshold.\n",
    "            threshold = point\n",
    "            if signal[index+1]< threshold and (signal[index]-signal[index-1]) > 0:# If the next value is lower than the threshold, then we add the point as detected and we restart the threshold.\n",
    "                if point > signal[index-1] and point > signal[index+1]:\n",
    "                    y_points.append(point) # We add the difference between the values because the first condition will detect all the points on the QRS complex, we need to not save the lower points on the peak's slope.\n",
    "                    x_points.append(index)\n",
    "        if point < temp: # Once the peak is stored, we restart the threshold value to calculate the following peak.\n",
    "            threshold = temp\n",
    "            \n",
    "    diff = np.diff(x_points)\n",
    "    x_del = []\n",
    "    y_del = []\n",
    "    for index,value in enumerate(diff):\n",
    "        if value < 200:\n",
    "            x_del = np.append(x_del,index)\n",
    "    \n",
    "    x_del = [ int(val) for val in x_del ]\n",
    "   \n",
    "    x_points = np.delete(x_points,x_del)\n",
    "    y_points = np.delete(y_points,x_del)\n",
    "\n",
    "    peaks['x_values'] = x_points\n",
    "    peaks['y_values'] = y_points\n",
    "    \n",
    "    return peaks\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a8ec3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _peakcorrection(peaks,or_signal,rate):\n",
    "    \"\"\"\n",
    "    Function that correct the detected peaks to be found exactly at the maximum point of the QRS complex by using a slinding window.\n",
    "    Inputs: peaks - the dictionary containing the detected peaks; or_signal - the original signal inside a 1D array; rate - sampling rate of the signal.\n",
    "    Output: peask - corrected peaks inside a dictionary\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Pin point exact qrs peak\n",
    "    window_check = int(rate/6)\n",
    "    #signal_normed = np.absolute((signal-np.mean(signal))/(max(signal)-min(signal)))\n",
    "    r_peaks = [0]*len(peaks['x_values'])\n",
    "    \n",
    "    for i,loc in enumerate(peaks['x_values']):\n",
    "        start = max(0,loc-window_check)\n",
    "        end = min(len(or_signal),loc+window_check)\n",
    "        wdw = np.absolute(or_signal[start:end] - np.mean(or_signal[start:end]))\n",
    "        pk = np.argmax(wdw)\n",
    "        r_peaks[i] = start+pk\n",
    "\n",
    "    y_values = []\n",
    "    for value in r_peaks:\n",
    "        y_values.append(or_signal[value])\n",
    "        \n",
    "    peaks = pd.DataFrame()\n",
    "    peaks['x_values'] = r_peaks\n",
    "    peaks['y_values'] = y_values\n",
    "    \n",
    "    return peaks\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd727e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filteringdet(signal,rate):\n",
    "    \"\"\"\n",
    "    Function wich combines all the filtering and peak detection functions in one module.\n",
    "    \"\"\"\n",
    "    normalised = (signal - np.min(signal)) / np.max(signal-np.min(signal))\n",
    "    filtered = _filtering(signal,rate)\n",
    "    peaks = _peakdetection(filtered)\n",
    "    n_peaks = _peakcorrection(peaks,signal,rate)\n",
    "    \n",
    "    return n_peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e646919c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
